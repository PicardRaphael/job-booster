# üéõÔ∏è Configuration LLM Avanc√©e

## üìã Vue d'ensemble

JobBooster permet de configurer finement les param√®tres LLM pour chaque agent individuellement via :
1. **Fichier YAML** : `app/agents/config/llm_config.yaml` (configuration de base)
2. **Variables d'environnement** : Surcharge dynamique via `AGENT_<NAME>_<PARAM>` (optionnel)

Chaque agent peut utiliser **n'importe quel provider** (OpenAI, Google, Anthropic) avec une configuration compl√®tement personnalis√©e.

## üìÅ Fichier de Configuration

### Structure (Nouveau Format)

```yaml
# Configuration par agent (n'importe quel provider)
agents:
  analyzer:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.3
    max_tokens: 1500
    top_p: 0.9
    frequency_penalty: 0.0
    presence_penalty: 0.0

  email_writer:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.7
    max_tokens: 1000
    top_p: 0.95
    frequency_penalty: 0.3
    presence_penalty: 0.2

  linkedin_writer:
    provider: google
    model: gemini-1.5-pro
    temperature: 0.75
    max_output_tokens: 800
    top_p: 0.95
    top_k: 40

  letter_writer:
    provider: anthropic  # Anthropic Claude support√© !
    model: claude-3-5-sonnet-20241022
    temperature: 0.8
    max_tokens: 2500
    top_p: 0.95

# Configuration par d√©faut
default:
  provider: openai
  model: gpt-4o-mini
  temperature: 0.7
  max_tokens: 2000
  top_p: 1.0
```

## üåç Providers Support√©s

### 1. OpenAI (GPT-4o-mini, GPT-4, etc.)

| Param√®tre | Type | Plage | Description |
|-----------|------|-------|-------------|
| `provider` | string | `openai` | Provider OpenAI |
| `model` | string | - | `gpt-4o-mini`, `gpt-4o`, `gpt-4-turbo` |
| `temperature` | float | 0.0 - 2.0 | Contr√¥le la cr√©ativit√© |
| `max_tokens` | int | 1 - 4096+ | Nombre max de tokens g√©n√©r√©s |
| `top_p` | float | 0.0 - 1.0 | Nucleus sampling |
| `frequency_penalty` | float | -2.0 - 2.0 | P√©nalise les r√©p√©titions |
| `presence_penalty` | float | -2.0 - 2.0 | Encourage nouveaux sujets |

### 2. Google Gemini

| Param√®tre | Type | Plage | Description |
|-----------|------|-------|-------------|
| `provider` | string | `google` | Provider Google |
| `model` | string | - | `gemini-1.5-pro`, `gemini-1.5-flash` |
| `temperature` | float | 0.0 - 2.0 | Contr√¥le la cr√©ativit√© |
| `max_output_tokens` | int | 1 - 8192+ | Nombre max de tokens g√©n√©r√©s |
| `top_p` | float | 0.0 - 1.0 | Nucleus sampling |
| `top_k` | int | 1 - 100 | Nombre de tokens consid√©r√©s |

### 3. Anthropic Claude

| Param√®tre | Type | Plage | Description |
|-----------|------|-------|-------------|
| `provider` | string | `anthropic` | Provider Anthropic |
| `model` | string | - | `claude-3-5-sonnet-20241022`, `claude-3-opus` |
| `temperature` | float | 0.0 - 1.0 | Contr√¥le la cr√©ativit√© |
| `max_tokens` | int | 1 - 4096+ | Nombre max de tokens g√©n√©r√©s |
| `top_p` | float | 0.0 - 1.0 | Nucleus sampling |

## üéØ Recommandations par Agent

### Analyzer (Extraction d'informations)

```yaml
analyzer:
  temperature: 0.3      # D√©terministe
  max_tokens: 1500      # Suffisant pour analyse
  top_p: 0.9
  frequency_penalty: 0.0
  presence_penalty: 0.0
```

**Pourquoi ?**
- **Temperature basse (0.3)** : Extraction factuelle, pas de cr√©ativit√© n√©cessaire
- **top_p 0.9** : R√©sultats coh√©rents et pr√©visibles

### Email Writer (Contenu professionnel court)

```yaml
email_writer:
  temperature: 0.7      # √âquilibr√©
  max_tokens: 1000      # Email concis
  top_p: 0.95
  frequency_penalty: 0.3  # √âvite r√©p√©titions
  presence_penalty: 0.2
```

**Pourquoi ?**
- **Temperature moyenne (0.7)** : Balance entre cr√©ativit√© et coh√©rence
- **frequency_penalty 0.3** : √âvite phrases r√©p√©titives
- **max_tokens 1000** : Limite pour email court

### LinkedIn Writer (Message engageant)

```yaml
linkedin_writer:
  temperature: 0.75     # L√©g√®rement cr√©atif
  max_output_tokens: 800
  top_p: 0.95
  top_k: 40
```

**Pourquoi ?**
- **Temperature 0.75** : Ton conversationnel mais pro
- **max_output_tokens 800** : Message LinkedIn court

### Letter Writer (Lettre longue et structur√©e)

```yaml
letter_writer:
  temperature: 0.8      # Plus cr√©atif
  max_tokens: 2500      # Lettre compl√®te
  top_p: 0.95
  frequency_penalty: 0.2
  presence_penalty: 0.3
```

**Pourquoi ?**
- **Temperature 0.8** : Cr√©ativit√© pour lettre engageante
- **max_tokens 2500** : Lettre de 300-400 mots
- **presence_penalty 0.3** : Diversit√© des sujets abord√©s

## üìä Guide des Temp√©ratures

```
0.0 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 2.0
‚îÇ         ‚îÇ         ‚îÇ         ‚îÇ         ‚îÇ
Factuel  √âquilibr√© Cr√©atif  Tr√®s      Chaos
                            cr√©atif

Cas d'usage :
‚îú‚îÄ 0.0-0.3 : Extraction, analyse, JSON
‚îú‚îÄ 0.4-0.7 : Contenu professionnel g√©n√©ral
‚îú‚îÄ 0.8-1.2 : Contenu cr√©atif, storytelling
‚îî‚îÄ 1.3-2.0 : Brainstorming, id√©es originales
```

## üîß Utilisation dans le Code

### Cr√©ation automatique via LLMFactory

```python
from app.core.llm_factory import get_llm_factory

# Obtenir la factory
llm_factory = get_llm_factory()

# Cr√©er LLM pour un agent sp√©cifique (d√©tecte automatiquement le provider)
analyzer_llm = llm_factory.create_llm_for_agent("analyzer")
# ‚Üí Lit llm_config.yaml et cr√©e le bon LLM (OpenAI, Google ou Anthropic)

email_llm = llm_factory.create_llm_for_agent("email_writer")
# ‚Üí Peut √™tre un provider diff√©rent !
```

### Dans JobApplicationCrew

```python
@agent
def analyzer(self) -> Agent:
    return Agent(
        config=self.agents_config["analyzer"],
        llm=self.llm_factory.create_llm_for_agent("analyzer"),
    )

@agent
def linkedin_writer(self) -> Agent:
    return Agent(
        config=self.agents_config["linkedin_writer"],
        llm=self.llm_factory.create_llm_for_agent("linkedin_writer"),
    )
```

## üîÑ Surcharge via Variables d'Environnement

### Format

```bash
AGENT_<AGENT_NAME>_<PARAM>=value
```

### Exemples

```bash
# Changer le provider de l'analyzer vers Anthropic
AGENT_ANALYZER_PROVIDER=anthropic
AGENT_ANALYZER_MODEL=claude-3-5-sonnet-20241022
AGENT_ANALYZER_TEMPERATURE=0.2

# Augmenter max_tokens pour email_writer
AGENT_EMAIL_WRITER_MAX_TOKENS=1500

# Utiliser GPT-4 pour letter_writer
AGENT_LETTER_WRITER_MODEL=gpt-4o
AGENT_LETTER_WRITER_TEMPERATURE=0.9
```

### Hi√©rarchie de Configuration

```
default (YAML)
    ‚Üì
agents.<agent_name> (YAML)
    ‚Üì
AGENT_<NAME>_<PARAM> (ENV)  ‚Üê Priorit√© la plus haute
```

**Exemple :**
```yaml
# llm_config.yaml
agents:
  analyzer:
    provider: openai
    temperature: 0.3
```

```bash
# .env
AGENT_ANALYZER_PROVIDER=anthropic  # Override le provider
AGENT_ANALYZER_TEMPERATURE=0.2      # Override la temp√©rature
```

**R√©sultat :** L'analyzer utilisera Claude avec temp√©rature 0.2

## üé® Personnalisation

### Ajouter un nouvel agent

1. **D√©finir dans `llm_config.yaml` :**

```yaml
agents:
  resume_analyzer:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.2    # Tr√®s factuel
    max_tokens: 2000
    top_p: 0.85
```

2. **Utiliser dans le code :**

```python
llm = self.llm_factory.create_llm_for_agent("resume_analyzer")
```

### Changer de provider facilement

**Via YAML :**
```yaml
agents:
  analyzer:
    provider: google  # Chang√© de openai √† google
    model: gemini-1.5-pro
    temperature: 0.3
```

**Via ENV :**
```bash
AGENT_ANALYZER_PROVIDER=anthropic
AGENT_ANALYZER_MODEL=claude-3-5-sonnet-20241022
```

## üß™ Tests de Configuration

### V√©rifier le chargement

```bash
# Lancer Python dans le container
docker exec -it jobbooster-backend python

>>> from app.core.llm_factory import get_llm_factory
>>> factory = get_llm_factory()
>>> config = factory._get_agent_config("analyzer")
>>> print(config)
{'provider': 'openai', 'model': 'gpt-4o-mini', 'temperature': 0.3, ...}
```

### Tester un agent

```python
from app.agents.crews import JobApplicationCrew

crew = JobApplicationCrew()
analyzer = crew.analyzer()

# V√©rifier la config LLM
print(analyzer.llm.__class__.__name__)  # ChatOpenAI, ChatGoogleGenerativeAI ou ChatAnthropic
print(analyzer.llm.temperature)  # 0.3
print(analyzer.llm.max_tokens)   # 1500
```

### Tester les ENV overrides

```bash
# Avec override
AGENT_ANALYZER_PROVIDER=anthropic \
AGENT_ANALYZER_TEMPERATURE=0.1 \
docker exec -it jobbooster-backend python -c "
from app.core.llm_factory import get_llm_factory
factory = get_llm_factory()
llm = factory.create_llm_for_agent('analyzer')
print(f'Provider: {llm.__class__.__name__}')
print(f'Temperature: {llm.temperature}')
"
```

## üö® Erreurs Communes

### 1. Configuration non trouv√©e

**Erreur :**
```
llm_config_not_found: Using defaults
```

**Solution :**
- V√©rifier que `app/agents/config/llm_config.yaml` existe
- V√©rifier les permissions du fichier

### 2. Agent non d√©fini

Si agent non trouv√© dans YAML ‚Üí utilise config `default`

```python
# "unknown_agent" n'existe pas dans YAML
llm = factory.create_llm_for_agent("unknown_agent")
# ‚Üí Utilise "default" automatiquement (provider: openai par d√©faut)
```

### 3. Provider non support√©

**Erreur :**
```
ValueError: Unsupported LLM provider: mistral
```

**Solution :**
- V√©rifier que le provider est bien `openai`, `google` ou `anthropic`
- Ajouter le support du nouveau provider dans `LLMFactory`

### 4. Param√®tre invalide

**Erreur :**
```
ValueError: temperature must be between 0 and 2
```

**Solution :**
- V√©rifier les plages de valeurs dans ce document
- Corriger dans `llm_config.yaml` ou ENV

### 5. API Key manquante

**Erreur :**
```
ValidationError: ANTHROPIC_API_KEY field required
```

**Solution :**
```bash
# Ajouter la cl√© API dans .env
ANTHROPIC_API_KEY=sk-ant-api03-...
```

## üìà Monitoring des Param√®tres

### Logs structur√©s

```json
{
  "event": "creating_llm_for_agent",
  "agent_name": "analyzer",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "temperature": 0.3,
  "timestamp": "2025-10-03T..."
}
```

```json
{
  "event": "env_override_applied",
  "agent_name": "analyzer",
  "param": "temperature",
  "value": 0.2,
  "timestamp": "2025-10-03T..."
}
```

### Langfuse

Tous les appels LLM sont trac√©s avec leurs param√®tres :
- Temperature utilis√©e
- Tokens g√©n√©r√©s
- Co√ªt estim√©
- Latence

## üí° Best Practices

1. **Commencer avec les defaults** puis ajuster
2. **Temperature basse** pour t√¢ches factuelles
3. **Temperature moyenne-haute** pour cr√©ativit√©
4. **max_tokens adapt√©** au type de contenu
5. **Choisir le bon provider** selon le cas d'usage :
   - OpenAI : Rapide, fiable, bon rapport qualit√©/prix
   - Google Gemini : Long contexte, multimodal
   - Anthropic Claude : Raisonnement complexe, s√©curit√©
6. **ENV pour surcharge temporaire** (dev/test)
7. **YAML pour configuration stable** (production)
8. **Tester et it√©rer** selon les r√©sultats
9. **Monitor via Langfuse** pour optimiser

## üöÄ Cas d'Usage Avanc√©s

### Utiliser diff√©rents providers par agent

```yaml
agents:
  analyzer:
    provider: anthropic  # Claude pour analyse pr√©cise
    model: claude-3-5-sonnet-20241022
    temperature: 0.2

  email_writer:
    provider: openai  # GPT pour g√©n√©ration rapide
    model: gpt-4o-mini
    temperature: 0.7

  linkedin_writer:
    provider: google  # Gemini pour ton conversationnel
    model: gemini-1.5-pro
    temperature: 0.75
```

### A/B testing via ENV

```bash
# Test A : OpenAI
AGENT_EMAIL_WRITER_PROVIDER=openai
AGENT_EMAIL_WRITER_MODEL=gpt-4o-mini

# Test B : Claude
AGENT_EMAIL_WRITER_PROVIDER=anthropic
AGENT_EMAIL_WRITER_MODEL=claude-3-5-sonnet-20241022
```

## üîó Ressources

- [OpenAI API Parameters](https://platform.openai.com/docs/api-reference/chat/create)
- [Gemini API Parameters](https://ai.google.dev/docs/gemini_api_overview)
- [Anthropic API Parameters](https://docs.anthropic.com/claude/reference/messages_post)
- [Temperature Explained](https://platform.openai.com/docs/guides/text-generation/parameter-details)

## üìù R√©sum√©

**Configuration flexible des LLM par agent :**
- ‚úÖ Fichier YAML pour config de base
- ‚úÖ Variables ENV pour surcharge dynamique
- ‚úÖ Support multi-providers (OpenAI, Google, Anthropic)
- ‚úÖ Hi√©rarchie claire : default < agent < ENV
- ‚úÖ Logs structur√©s pour debugging
- ‚úÖ Monitoring Langfuse int√©gr√©

**Format ENV :**
```bash
AGENT_<AGENT_NAME>_<PARAM>=value
```

**Exemple complet :**
```yaml
# llm_config.yaml
agents:
  analyzer:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.3
```

```bash
# .env (override)
AGENT_ANALYZER_PROVIDER=anthropic
AGENT_ANALYZER_MODEL=claude-3-5-sonnet-20241022
AGENT_ANALYZER_TEMPERATURE=0.2
```

---

**Documentation par** : Team JobBooster
**Derni√®re mise √† jour** : 2025-10-03
