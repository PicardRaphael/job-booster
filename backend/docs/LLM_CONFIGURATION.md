# üéõÔ∏è Configuration LLM Avanc√©e - Clean Architecture

## üìã Vue d'ensemble

JobBooster utilise une architecture **Clean Architecture** avec multi-provider LLM.

La configuration LLM se fait via :
1. **Fichier YAML** : `app/agents/config/llm_config.yaml` (configuration de base)
2. **Variables d'environnement** : Surcharge dynamique via `AGENT_<NAME>_<PARAM>` (optionnel)

Chaque agent peut utiliser **n'importe quel provider** (OpenAI, Google, Anthropic) avec une configuration compl√®tement personnalis√©e.

## üèóÔ∏è Architecture

```
LLMFactory (Core)
    ‚Üì
LLMProviderAdapter (Infrastructure)
    ‚Üì
CrewAI Adapters (Infrastructure)
    ‚Üì
Use Cases (Application)
```

La **LLMFactory** cr√©e des LLMs configur√©s qui sont inject√©s dans les **adapters** via le **Container** (Dependency Injection).

## üìÅ Fichier de Configuration

### Structure (Nouveau Format)

```yaml
# Configuration par agent (n'importe quel provider)
agents:
  analyzer:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.3
    max_tokens: 1500
    top_p: 0.9
    frequency_penalty: 0.0
    presence_penalty: 0.0

  email_writer:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.7
    max_tokens: 1000
    top_p: 0.95
    frequency_penalty: 0.3
    presence_penalty: 0.2

  linkedin_writer:
    provider: google
    model: gemini-1.5-pro
    temperature: 0.75
    max_output_tokens: 800
    top_p: 0.95
    top_k: 40

  letter_writer:
    provider: anthropic  # Anthropic Claude support√© !
    model: claude-3-5-sonnet-20241022
    temperature: 0.8
    max_tokens: 2500
    top_p: 0.95

# Configuration par d√©faut
default:
  provider: openai
  model: gpt-4o-mini
  temperature: 0.7
  max_tokens: 2000
  top_p: 1.0
```

## üåç Providers Support√©s

### 1. OpenAI (GPT-4o-mini, GPT-4, etc.)

| Param√®tre | Type | Plage | Description |
|-----------|------|-------|-------------|
| `provider` | string | `openai` | Provider OpenAI |
| `model` | string | - | `gpt-4o-mini`, `gpt-4o`, `gpt-4-turbo` |
| `temperature` | float | 0.0 - 2.0 | Contr√¥le la cr√©ativit√© |
| `max_tokens` | int | 1 - 4096+ | Nombre max de tokens g√©n√©r√©s |
| `top_p` | float | 0.0 - 1.0 | Nucleus sampling |
| `frequency_penalty` | float | -2.0 - 2.0 | P√©nalise les r√©p√©titions |
| `presence_penalty` | float | -2.0 - 2.0 | Encourage nouveaux sujets |

### 2. Google Gemini

| Param√®tre | Type | Plage | Description |
|-----------|------|-------|-------------|
| `provider` | string | `google` | Provider Google |
| `model` | string | - | `gemini-1.5-pro`, `gemini-1.5-flash` |
| `temperature` | float | 0.0 - 2.0 | Contr√¥le la cr√©ativit√© |
| `max_output_tokens` | int | 1 - 8192+ | Nombre max de tokens g√©n√©r√©s |
| `top_p` | float | 0.0 - 1.0 | Nucleus sampling |
| `top_k` | int | 1 - 100 | Nombre de tokens consid√©r√©s |

### 3. Anthropic Claude

| Param√®tre | Type | Plage | Description |
|-----------|------|-------|-------------|
| `provider` | string | `anthropic` | Provider Anthropic |
| `model` | string | - | `claude-3-5-sonnet-20241022`, `claude-3-opus` |
| `temperature` | float | 0.0 - 1.0 | Contr√¥le la cr√©ativit√© |
| `max_tokens` | int | 1 - 4096+ | Nombre max de tokens g√©n√©r√©s |
| `top_p` | float | 0.0 - 1.0 | Nucleus sampling |

## üéØ Recommandations par Agent

### Analyzer (Extraction d'informations)

```yaml
analyzer:
  temperature: 0.3      # D√©terministe
  max_tokens: 1500      # Suffisant pour analyse
  top_p: 0.9
  frequency_penalty: 0.0
  presence_penalty: 0.0
```

**Pourquoi ?**
- **Temperature basse (0.3)** : Extraction factuelle, pas de cr√©ativit√© n√©cessaire
- **top_p 0.9** : R√©sultats coh√©rents et pr√©visibles

### Email Writer (Contenu professionnel court)

```yaml
email_writer:
  temperature: 0.7      # √âquilibr√©
  max_tokens: 1000      # Email concis
  top_p: 0.95
  frequency_penalty: 0.3  # √âvite r√©p√©titions
  presence_penalty: 0.2
```

**Pourquoi ?**
- **Temperature moyenne (0.7)** : Balance entre cr√©ativit√© et coh√©rence
- **frequency_penalty 0.3** : √âvite phrases r√©p√©titives
- **max_tokens 1000** : Limite pour email court

### LinkedIn Writer (Message engageant)

```yaml
linkedin_writer:
  temperature: 0.75     # L√©g√®rement cr√©atif
  max_output_tokens: 800
  top_p: 0.95
  top_k: 40
```

**Pourquoi ?**
- **Temperature 0.75** : Ton conversationnel mais pro
- **max_output_tokens 800** : Message LinkedIn court

### Letter Writer (Lettre longue et structur√©e)

```yaml
letter_writer:
  temperature: 0.8      # Plus cr√©atif
  max_tokens: 2500      # Lettre compl√®te
  top_p: 0.95
  frequency_penalty: 0.2
  presence_penalty: 0.3
```

**Pourquoi ?**
- **Temperature 0.8** : Cr√©ativit√© pour lettre engageante
- **max_tokens 2500** : Lettre de 300-400 mots
- **presence_penalty 0.3** : Diversit√© des sujets abord√©s

## üìä Guide des Temp√©ratures

```
0.0 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 2.0
‚îÇ         ‚îÇ         ‚îÇ         ‚îÇ         ‚îÇ
Factuel  √âquilibr√© Cr√©atif  Tr√®s      Chaos
                            cr√©atif

Cas d'usage :
‚îú‚îÄ 0.0-0.3 : Extraction, analyse, JSON
‚îú‚îÄ 0.4-0.7 : Contenu professionnel g√©n√©ral
‚îú‚îÄ 0.8-1.2 : Contenu cr√©atif, storytelling
‚îî‚îÄ 1.3-2.0 : Brainstorming, id√©es originales
```

## üîß Utilisation dans le Code (Clean Architecture)

### Via le Container (Dependency Injection)

```python
from app.core.container import get_container

# Le container g√®re toutes les d√©pendances
container = get_container()

# LLMFactory est inject√© dans les adapters
llm_provider = container.llm_provider()  # LLMProviderAdapter

# Les adapters utilisent le LLMProvider
email_writer_adapter = container.content_writer_service().get_email_writer()
# ‚Üí Cr√©√© avec LLM configur√© automatiquement
```

### Dans les Adapters (Infrastructure)

```python
# app/infrastructure/ai/crewai/email_writer_adapter.py
class EmailWriterAdapter(IEmailWriter):
    def __init__(self, llm_provider: ILLMProvider, agent_config: Dict, task_config: Dict):
        self.llm_provider = llm_provider
        self.agent_config = agent_config
        self.task_config = task_config

    def write_email(self, job_offer, analysis, context):
        # Cr√©er LLM via provider (d√©tecte automatiquement le provider YAML)
        llm = self.llm_provider.create_llm("email_writer")

        # Cr√©er agent avec LLM configur√©
        agent = AgentBuilder().from_config(self.agent_config).with_llm(llm).build()

        # Cr√©er task
        task = Task(description=self.task_config["description"], agent=agent)

        # Cr√©er crew
        crew = CrewBuilder().add_agent(agent).add_task(task).build()

        # Ex√©cuter
        result = crew.kickoff(inputs={...})
        return str(result)
```

### LLMFactory (Core Layer)

```python
# app/core/llm_factory.py
class LLMFactory:
    def __init__(self, llm_config: Dict[str, Any]):
        """Config inject√©e, pas charg√©e (Clean Architecture)."""
        self.config = llm_config

    def create_llm_for_agent(self, agent_name: str) -> BaseChatModel:
        """Cr√©e LLM pour agent (d√©tecte provider automatiquement)."""
        config = self._get_agent_config(agent_name)
        provider = config.get("provider", "openai")

        if provider == "openai":
            return self._create_openai_llm(config)
        elif provider == "google":
            return self._create_google_llm(config)
        elif provider == "anthropic":
            return self._create_anthropic_llm(config)
```

## üîÑ Surcharge via Variables d'Environnement

### Format

```bash
AGENT_<AGENT_NAME>_<PARAM>=value
```

### Exemples

```bash
# Changer le provider de l'analyzer vers Anthropic
AGENT_ANALYZER_PROVIDER=anthropic
AGENT_ANALYZER_MODEL=claude-3-5-sonnet-20241022
AGENT_ANALYZER_TEMPERATURE=0.2

# Augmenter max_tokens pour email_writer
AGENT_EMAIL_WRITER_MAX_TOKENS=1500

# Utiliser GPT-4 pour letter_writer
AGENT_LETTER_WRITER_MODEL=gpt-4o
AGENT_LETTER_WRITER_TEMPERATURE=0.9
```

### Hi√©rarchie de Configuration

```
default (YAML)
    ‚Üì
agents.<agent_name> (YAML)
    ‚Üì
AGENT_<NAME>_<PARAM> (ENV)  ‚Üê Priorit√© la plus haute
```

**Exemple :**
```yaml
# llm_config.yaml
agents:
  analyzer:
    provider: openai
    temperature: 0.3
```

```bash
# .env
AGENT_ANALYZER_PROVIDER=anthropic  # Override le provider
AGENT_ANALYZER_TEMPERATURE=0.2      # Override la temp√©rature
```

**R√©sultat :** L'analyzer utilisera Claude avec temp√©rature 0.2

## üé® Personnalisation

### Ajouter un nouvel agent

1. **D√©finir dans `llm_config.yaml` :**

```yaml
agents:
  resume_analyzer:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.2    # Tr√®s factuel
    max_tokens: 2000
    top_p: 0.85
```

2. **Utiliser dans le code :**

```python
llm = self.llm_factory.create_llm_for_agent("resume_analyzer")
```

### Changer de provider facilement

**Via YAML :**
```yaml
agents:
  analyzer:
    provider: google  # Chang√© de openai √† google
    model: gemini-1.5-pro
    temperature: 0.3
```

**Via ENV :**
```bash
AGENT_ANALYZER_PROVIDER=anthropic
AGENT_ANALYZER_MODEL=claude-3-5-sonnet-20241022
```

## üß™ Tests de Configuration

### V√©rifier le chargement via Container

```bash
# Lancer Python dans le container
docker exec -it jobbooster-backend python

>>> from app.core.container import get_container
>>> container = get_container()
>>> llm_provider = container.llm_provider()
>>> llm = llm_provider.create_llm("analyzer")
>>> print(llm.__class__.__name__)  # ChatOpenAI, ChatGoogleGenerativeAI ou ChatAnthropic
>>> print(llm.temperature)  # 0.3
```

### Tester un Use Case

```python
from app.core.container import get_container

# Obtenir orchestrator (contient tous les use cases)
container = get_container()
orchestrator = container.generate_application_orchestrator()

# V√©rifier les LLMs utilis√©s
analyzer_service = container.analyzer_service()
# ‚Üí CrewAIAnalyzerAdapter avec LLM configur√©

email_writer = container.content_writer_service().get_email_writer()
# ‚Üí EmailWriterAdapter avec LLM configur√©
```

### Tester les ENV overrides

```bash
# Avec override
AGENT_ANALYZER_PROVIDER=anthropic \
AGENT_ANALYZER_TEMPERATURE=0.1 \
docker exec -it jobbooster-backend python -c "
from app.core.container import get_container
container = get_container()
llm_provider = container.llm_provider()
llm = llm_provider.create_llm('analyzer')
print(f'Provider: {llm.__class__.__name__}')
print(f'Temperature: {llm.temperature}')
"
```

## üö® Erreurs Communes

### 1. Configuration non trouv√©e

**Erreur :**
```
llm_config_not_found: Using defaults
```

**Solution :**
- V√©rifier que `app/agents/config/llm_config.yaml` existe
- V√©rifier les permissions du fichier

### 2. Agent non d√©fini

Si agent non trouv√© dans YAML ‚Üí utilise config `default`

```python
# "unknown_agent" n'existe pas dans YAML
llm = factory.create_llm_for_agent("unknown_agent")
# ‚Üí Utilise "default" automatiquement (provider: openai par d√©faut)
```

### 3. Provider non support√©

**Erreur :**
```
ValueError: Unsupported LLM provider: mistral
```

**Solution :**
- V√©rifier que le provider est bien `openai`, `google` ou `anthropic`
- Ajouter le support du nouveau provider dans `LLMFactory`

### 4. Param√®tre invalide

**Erreur :**
```
ValueError: temperature must be between 0 and 2
```

**Solution :**
- V√©rifier les plages de valeurs dans ce document
- Corriger dans `llm_config.yaml` ou ENV

### 5. API Key manquante

**Erreur :**
```
ValidationError: ANTHROPIC_API_KEY field required
```

**Solution :**
```bash
# Ajouter la cl√© API dans .env
ANTHROPIC_API_KEY=sk-ant-api03-...
```

## üìà Monitoring des Param√®tres

### Logs structur√©s

```json
{
  "event": "creating_llm_for_agent",
  "agent_name": "analyzer",
  "provider": "openai",
  "model": "gpt-4o-mini",
  "temperature": 0.3,
  "timestamp": "2025-10-03T..."
}
```

```json
{
  "event": "env_override_applied",
  "agent_name": "analyzer",
  "param": "temperature",
  "value": 0.2,
  "timestamp": "2025-10-03T..."
}
```

### Langfuse

Tous les appels LLM sont trac√©s avec leurs param√®tres :
- Temperature utilis√©e
- Tokens g√©n√©r√©s
- Co√ªt estim√©
- Latence

## üí° Best Practices

1. **Commencer avec les defaults** puis ajuster
2. **Temperature basse** pour t√¢ches factuelles
3. **Temperature moyenne-haute** pour cr√©ativit√©
4. **max_tokens adapt√©** au type de contenu
5. **Choisir le bon provider** selon le cas d'usage :
   - OpenAI : Rapide, fiable, bon rapport qualit√©/prix
   - Google Gemini : Long contexte, multimodal
   - Anthropic Claude : Raisonnement complexe, s√©curit√©
6. **ENV pour surcharge temporaire** (dev/test)
7. **YAML pour configuration stable** (production)
8. **Tester et it√©rer** selon les r√©sultats
9. **Monitor via Langfuse** pour optimiser

## üöÄ Cas d'Usage Avanc√©s

### Utiliser diff√©rents providers par agent

```yaml
agents:
  analyzer:
    provider: anthropic  # Claude pour analyse pr√©cise
    model: claude-3-5-sonnet-20241022
    temperature: 0.2

  email_writer:
    provider: openai  # GPT pour g√©n√©ration rapide
    model: gpt-4o-mini
    temperature: 0.7

  linkedin_writer:
    provider: google  # Gemini pour ton conversationnel
    model: gemini-1.5-pro
    temperature: 0.75
```

### A/B testing via ENV

```bash
# Test A : OpenAI
AGENT_EMAIL_WRITER_PROVIDER=openai
AGENT_EMAIL_WRITER_MODEL=gpt-4o-mini

# Test B : Claude
AGENT_EMAIL_WRITER_PROVIDER=anthropic
AGENT_EMAIL_WRITER_MODEL=claude-3-5-sonnet-20241022
```

## üîó Ressources

- [OpenAI API Parameters](https://platform.openai.com/docs/api-reference/chat/create)
- [Gemini API Parameters](https://ai.google.dev/docs/gemini_api_overview)
- [Anthropic API Parameters](https://docs.anthropic.com/claude/reference/messages_post)
- [Temperature Explained](https://platform.openai.com/docs/guides/text-generation/parameter-details)

## üìù R√©sum√©

**Configuration flexible des LLM par agent (Clean Architecture) :**
- ‚úÖ Fichier YAML pour config de base
- ‚úÖ Variables ENV pour surcharge dynamique
- ‚úÖ Support multi-providers (OpenAI, Google, Anthropic)
- ‚úÖ Hi√©rarchie claire : default < agent < ENV
- ‚úÖ Dependency Injection via Container
- ‚úÖ LLMFactory inject√© dans adapters (pas chargement I/O)
- ‚úÖ Logs structur√©s pour debugging
- ‚úÖ Monitoring Langfuse int√©gr√©

**Architecture :**
```
YAMLConfigurationLoader (Infrastructure)
    ‚Üì Charge config
LLMFactory (Core)
    ‚Üì Cr√©e LLMs
LLMProviderAdapter (Infrastructure)
    ‚Üì Inject√© dans
CrewAI Adapters (Infrastructure)
    ‚Üì Utilis√© par
Use Cases (Application)
```

**Format ENV :**
```bash
AGENT_<AGENT_NAME>_<PARAM>=value
```

**Exemple complet :**
```yaml
# llm_config.yaml
agents:
  analyzer:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.3

  email_writer:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.7

  linkedin_writer:
    provider: google
    model: gemini-1.5-pro
    temperature: 0.75

  letter_writer:
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    temperature: 0.8
```

```bash
# .env (override)
AGENT_ANALYZER_PROVIDER=anthropic
AGENT_ANALYZER_MODEL=claude-3-5-sonnet-20241022
AGENT_ANALYZER_TEMPERATURE=0.2
```

**Utilisation dans le code :**
```python
# Via Container (Dependency Injection)
from app.core.container import get_container

container = get_container()
orchestrator = container.generate_application_orchestrator()
# Tous les LLMs sont automatiquement configur√©s et inject√©s !
```

---

**Documentation par** : Team JobBooster
**Derni√®re mise √† jour** : 2025-10-04
**Architecture** : Clean Architecture + SOLID
